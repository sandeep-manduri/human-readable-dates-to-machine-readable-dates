# human-readable-dates-to-machine-readable-dates-using transformers-and-attention

example:
    
    feb 2nd 2000  is human readable and model translates it to   yyyy-mm-dd format  which is 2000-02-02
    
    
models in the notebook are 
1.  LSTM + multi_head_attention model

2. Transformer with stack length of both encoder and decoder as 3
